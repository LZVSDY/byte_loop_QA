import os
import json
import csv
import glob

BASE_RESULT_DIR = "/data1/lz/loop_QA/test1"  # ä¸“ä¸šè¯ç›®å½•çš„æ ¹è·¯å¾„
SOURCE_TXT_FILE = "/data1/lz/loop_QA/dataset/random_entries_sample.txt"     # åŒ…å«åŸå§‹ä¸“ä¸šè¯åˆ—è¡¨çš„txtæ–‡ä»¶è·¯å¾„
FAILURE_LOG_FILE = "/data1/lz/loop_QA/test_final/failure_sample.txt"             # è®°å½•å¤±è´¥ä¸“ä¸šè¯çš„æ–‡ä»¶
OUTPUT_CSV_FILE = "/data1/lz/loop_QA/test_final/qa_report_sample.csv"            # è¾“å‡ºçš„CSVæ–‡ä»¶å

def sanitize_to_dirname(term_name: str) -> str:
    """å°†åŸå§‹ä¸“ä¸šè¯è½¬æ¢ä¸ºç›®å½•åæ ¼å¼ (å°å†™ï¼Œç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿)ã€‚"""
    return term_name.replace(" ", "_").lower()

def dirname_to_original(dirname: str) -> str:
    """å°†ç›®å½•åæ ¼å¼è½¬æ¢å›åŸå§‹ä¸“ä¸šè¯ (ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œé¦–å­—æ¯å¯èƒ½éœ€è¦å¤§å†™ - æ ¹æ®åŸå§‹åˆ—è¡¨å†³å®š)ã€‚
       ä¸ºäº†æ›´å‡†ç¡®åœ°è¿˜åŸï¼Œæˆ‘ä»¬ä»åŸå§‹txtåˆ—è¡¨ä¸­æŸ¥æ‰¾ã€‚
    """
    # æ³¨æ„: è¿™ä¸ªå‡½æ•°ç°åœ¨ä¾èµ–äºåŸå§‹åˆ—è¡¨æ¥ç²¾ç¡®è¿˜åŸå¤§å°å†™å’ŒåŸå§‹ç©ºæ ¼
    # å¦‚æœåŸå§‹åˆ—è¡¨ä¸å¯ç”¨æˆ–ä¸å¸Œæœ›ä¾èµ–å®ƒï¼Œå¯ä»¥ç®€å•åœ°ç”¨replaceå’Œtitle()ç­‰ï¼Œä½†å¯èƒ½ä¸å®Œå…¨ä¸€è‡´
    # ä¾‹å¦‚: return dirname.replace("_", " ").title()
    # è¿™é‡Œæˆ‘ä»¬å‡è®¾åŸå§‹åˆ—è¡¨å·²ç»åŠ è½½åˆ°å†…å­˜ä¸­
    global original_terms_map
    return original_terms_map.get(dirname, dirname.replace("_", " "))


def process_directories():
    """
    å¤„ç†æ‰€æœ‰ä¸“ä¸šè¯ç›®å½•ï¼Œè®°å½•å¤±è´¥é¡¹å¹¶æå–QAå¯¹åˆ°CSVã€‚
    """
    failed_terms = []
    csv_data_rows = []

    # 1. è¯»å–åŸå§‹ä¸“ä¸šè¯åˆ—è¡¨ï¼Œç”¨äºå‡†ç¡®è¿˜åŸåç§°
    global original_terms_map
    original_terms_map = {} # dirname -> original_term
    try:
        with open(SOURCE_TXT_FILE, 'r', encoding='utf-8') as f_source:
            for line in f_source:
                original_term = line.strip()
                if original_term:
                    dir_name_version = sanitize_to_dirname(original_term)
                    original_terms_map[dir_name_version] = original_term
        print(f"âœ… æˆåŠŸä» {SOURCE_TXT_FILE} åŠ è½½äº† {len(original_terms_map)} ä¸ªåŸå§‹ä¸“ä¸šè¯æ¡ç›®ã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Š: åŸå§‹ä¸“ä¸šè¯åˆ—è¡¨æ–‡ä»¶ {SOURCE_TXT_FILE} æœªæ‰¾åˆ°ã€‚å°†å°è¯•ä»ç›®å½•åç›´æ¥è½¬æ¢ã€‚")
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™mapä¸ºç©ºï¼Œåç»­è½¬æ¢å°†ä½¿ç”¨ç®€å•æ›¿æ¢

    # 2. éå†BASE_RESULT_DIRä¸‹çš„æ‰€æœ‰å­ç›®å½•ï¼ˆè¿™äº›è¢«è®¤ä¸ºæ˜¯ä¸“ä¸šè¯ç›®å½•ï¼‰
    for term_dirname in os.listdir(BASE_RESULT_DIR):
        current_term_path = os.path.join(BASE_RESULT_DIR, term_dirname)

        if not os.path.isdir(current_term_path):
            continue # è·³è¿‡éç›®å½•é¡¹

        # è¿˜åŸä¸“ä¸šè¯çš„åŸå§‹åç§°
        original_term_name = dirname_to_original(term_dirname)

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ wikipedia*.txt æ–‡ä»¶
        """
        student_loop.json æ ¼å¼ç¤ºä¾‹ï¼š
        [
            {
                "question_text": "Which season of an English football club involved competition in a Southern league before the integration of that league into a higher division, occurred when the club was renamed from its original 19th-century name, and took place in the early 20th century?",
                "answer1": "\n\n1919â€“20",
                "check_result1": "\n\nno",
                "answer2": "\n\n1919-20",
                "check_result2": "\nno",
                "answer3": "\n\n1919-20",
                "check_result3": "\n\nno",
                "answer4": "\n\n1919â€“20",
                "check_result4": "\n\nno",
                "answer5": "\n\n1900-01",
                "check_result5": "\n\nno"
            }
        ]
        """
        wiki_files_pattern = os.path.join(current_term_path, "wikipedia*.txt")
        if not glob.glob(wiki_files_pattern): # glob.globè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è¡¨ç¤ºæ²¡æ‰¾åˆ°
            failed_terms.append(original_term_name)
            print(f"ğŸ“‰ ä¸“ä¸šè¯ '{original_term_name}' (ç›®å½•: {term_dirname}) å¤±è´¥ï¼Œç¼ºå°‘ wikipedia*.txt æ–‡ä»¶ã€‚")
            # å¯¹äºå¤±è´¥çš„æ¡ç›®ï¼Œæˆ‘ä»¬ä¸å†å°è¯•è¯»å– student_loop.json

        # å¦‚æœå­˜åœ¨ wikipedia*.txt æ–‡ä»¶ï¼Œåˆ™å¤„ç† student_loop.json
        
        
        else:
            student_answers_path = os.path.join(current_term_path, "student_loop.json")
            if os.path.exists(student_answers_path):
                try:
                    with open(student_answers_path, 'r', encoding='utf-8') as f_answers:
                        qa_pairs = json.load(f_answers)
                        if isinstance(qa_pairs, list): # ç¡®ä¿æ˜¯åˆ—è¡¨
                            for pair in qa_pairs:
                                qustion = pair.get("question_text")
                                answer1 = pair.get("answer1")
                                answer2 = pair.get("answer2")
                                answer3 = pair.get("answer3")
                                answer4 = pair.get("answer4")
                                answer5 = pair.get("answer5")
                                
                                check_result1 = pair.get("check_result1")
                                check_result2 = pair.get("check_result2")
                                check_result3 = pair.get("check_result3")
                                check_result4 = pair.get("check_result4")
                                check_result5 = pair.get("check_result5")
                                
                                if qustion and answer1 and answer2 and answer3 and answer4 \
                                    and answer5 and check_result1 and check_result2 and check_result3 \
                                    and check_result4 and check_result5: # ç¡®ä¿é”®å­˜åœ¨ä¸”å€¼ä¸ä¸ºç©º
                                    csv_data_rows.append({
                                        "Answer": original_term_name,
                                        "Question": qustion,
                                        "Model_Answer1": answer1,
                                        "Model_Answer2": answer2,
                                        "Model_Answer3": answer3,
                                        "Model_Answer4": answer4,
                                        "Model_Answer5": answer5,
                                        "Check_Result1": check_result1,
                                        "Check_Result2": check_result2,
                                        "Check_Result3": check_result3,
                                        "Check_Result4": check_result4,
                                        "Check_Result5": check_result5
                                    })
                                else:
                                    print(f"âš ï¸ è­¦å‘Š: åœ¨ {student_answers_path} ä¸­æ‰¾åˆ°ä¸å®Œæ•´çš„QAå¯¹: {pair}")
                                
                                # question = pair.get("question_text")
                                # model_answer = pair.get("answer")
                                # if question and model_answer: # ç¡®ä¿é”®å­˜åœ¨ä¸”å€¼ä¸ä¸ºç©º
                                #     csv_data_rows.append({
                                #         "Answer": original_term_name,
                                #         "Question": question,
                                #         "Model_Answer": model_answer
                                #     })
                                # else:
                                #     print(f"âš ï¸ è­¦å‘Š: åœ¨ {student_answers_path} ä¸­æ‰¾åˆ°ä¸å®Œæ•´çš„QAå¯¹: {pair}")
                        else:
                            print(f"âš ï¸ è­¦å‘Š: {student_answers_path} çš„å†…å®¹ä¸æ˜¯é¢„æœŸçš„åˆ—è¡¨æ ¼å¼ã€‚")
                except json.JSONDecodeError:
                    print(f"âŒ é”™è¯¯: è§£æJSONæ–‡ä»¶å¤±è´¥ {student_answers_path}")
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶ {student_answers_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            else:
                print(f"âš ï¸ è­¦å‘Š: ä¸“ä¸šè¯ '{original_term_name}' (ç›®å½•: {term_dirname}) çš„ student_answers.json æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")


    # 3. å°†å¤±è´¥çš„ä¸“ä¸šè¯å†™å…¥ failure.txt
    if failed_terms:
        with open(FAILURE_LOG_FILE, 'w', encoding='utf-8') as f_failure:
            for term in failed_terms:
                f_failure.write(term + "\n")
        print(f"\nğŸ“ å…±æœ‰ {len(failed_terms)} ä¸ªå¤±è´¥çš„ä¸“ä¸šè¯å·²å†™å…¥ {FAILURE_LOG_FILE}")
    else:
        print("\nâœ¨ æ‰€æœ‰ä¸“ä¸šè¯ç›®å½•å‡åŒ…å« wikipedia*.txt æ–‡ä»¶ï¼Œæ²¡æœ‰å¤±è´¥é¡¹ã€‚")

    # 4. å°†æå–çš„æ•°æ®å†™å…¥CSVæ–‡ä»¶
    if csv_data_rows:
        csv_fieldnames = ["Answer", "Question", "Model_Answer1", "Model_Answer2", "Model_Answer3", "Model_Answer4", "Model_Answer5",
                          "Check_Result1", "Check_Result2", "Check_Result3", "Check_Result4", "Check_Result5"]
        with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as f_csv:
            writer = csv.DictWriter(f_csv, fieldnames=csv_fieldnames)
            writer.writeheader()
            writer.writerows(csv_data_rows)
        print(f"ğŸ“Š æˆåŠŸå°† {len(csv_data_rows)} æ¡QAè®°å½•å†™å…¥ {OUTPUT_CSV_FILE}")
    else:
        print("\nâ„¹ï¸ æœªæå–åˆ°ä»»ä½•QAæ•°æ®ç”¨äºç”ŸæˆCSVæŠ¥å‘Šã€‚")

if __name__ == "__main__":
    # é¦–å…ˆï¼Œç¡®ä¿ä½ çš„åŸå§‹ä¸“ä¸šè¯txtæ–‡ä»¶å­˜åœ¨ï¼Œæˆ–è€…æ¥å—è­¦å‘Šå¹¶è¿›è¡Œåç»­å¤„ç†
    # å‡è®¾ä½ çš„åŸå§‹ä¸“ä¸šè¯æ–‡ä»¶åä¸º 'your_source_list.txt'ï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„ SOURCE_TXT_FILE å˜é‡
    # å¦‚æœä½ æ²¡æœ‰åŸå§‹åˆ—è¡¨ï¼Œè¿˜åŸçš„ Answer åç§°å¯èƒ½åœ¨å¤§å°å†™ä¸Šä¸åŸå§‹è¯æ¡ä¸å®Œå…¨ä¸€è‡´
    
    # ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„åŸå§‹åˆ—è¡¨æ˜¯ 'terms.txt'ï¼Œè¯·å°†ä¸Šé¢é…ç½®éƒ¨åˆ†çš„
    # SOURCE_TXT_FILE = "your_source_list.txt" ä¿®æ”¹ä¸º
    # SOURCE_TXT_FILE = "terms.txt"

    # è¿è¡Œä¸»å¤„ç†å‡½æ•°
    process_directories()

    print("\nğŸš€ å¤„ç†å®Œæˆï¼")